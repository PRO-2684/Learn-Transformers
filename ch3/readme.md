# ç¬¬ä¸‰ç« ï¼šæ³¨æ„åŠ›æœºåˆ¶

## Attention

NLP ç¥ç»ç½‘ç»œæ¨¡å‹çš„æœ¬è´¨å°±æ˜¯å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œå¸¸è§„çš„åšæ³•æ˜¯é¦–å…ˆå¯¹å¥å­è¿›è¡Œåˆ†è¯ï¼Œç„¶åå°†æ¯ä¸ªè¯è¯­ (token) éƒ½è½¬åŒ–ä¸ºå¯¹åº”çš„è¯å‘é‡ (token embeddings)ï¼Œè¿™æ ·æ–‡æœ¬å°±è½¬æ¢ä¸ºä¸€ä¸ªç”±è¯è¯­å‘é‡ç»„æˆçš„çŸ©é˜µ $\boldsymbol{X}=(\boldsymbol{x}_1,\boldsymbol{x}_2,\dots,\boldsymbol{x}_n)$ï¼Œå…¶ä¸­ $\boldsymbol{x}_i$ æ˜¯ç¬¬ $i$ ä¸ªè¯è¯­çš„è¯å‘é‡ï¼Œç»´åº¦ä¸º $d$ï¼Œæ•… $\boldsymbol{X}\in \mathbb{R}^{n\times d}$ã€‚

åœ¨ Transformer æ¨¡å‹æå‡ºä¹‹å‰ï¼Œå¯¹ token åºåˆ— $\boldsymbol{X}$ çš„å¸¸è§„ç¼–ç æ–¹å¼æ˜¯é€šè¿‡å¾ªç¯ç½‘ç»œ (RNNs) å’Œå·ç§¯ç½‘ç»œ (CNNs)ã€‚

- RNNï¼ˆä¾‹å¦‚ LSTMï¼‰çš„æ–¹æ¡ˆå¾ˆç®€å•ï¼Œæ¯ä¸€ä¸ªè¯è¯­ $\boldsymbol{x}_t$ å¯¹åº”çš„ç¼–ç ç»“æœ $\boldsymbol{y}_t$ é€šè¿‡é€’å½’åœ°è®¡ç®—å¾—åˆ°ï¼š$\boldsymbol{y}_t =f(\boldsymbol{y}_{t-1},\boldsymbol{x}_t)$ã€‚RNN çš„åºåˆ—å»ºæ¨¡æ–¹å¼è™½ç„¶ä¸äººç±»é˜…è¯»ç±»ä¼¼ï¼Œä½†æ˜¯é€’å½’çš„ç»“æ„å¯¼è‡´å…¶æ— æ³•å¹¶è¡Œè®¡ç®—ï¼Œå› æ­¤é€Ÿåº¦è¾ƒæ…¢ã€‚è€Œä¸” RNN æœ¬è´¨æ˜¯ä¸€ä¸ªé©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹ï¼Œéš¾ä»¥å­¦ä¹ åˆ°å…¨å±€çš„ç»“æ„ä¿¡æ¯ï¼›
- CNN åˆ™é€šè¿‡æ»‘åŠ¨çª—å£åŸºäºå±€éƒ¨ä¸Šä¸‹æ–‡æ¥ç¼–ç æ–‡æœ¬ï¼Œä¾‹å¦‚æ ¸å°ºå¯¸ä¸º 3 çš„å·ç§¯æ“ä½œå°±æ˜¯ä½¿ç”¨æ¯ä¸€ä¸ªè¯è‡ªèº«ä»¥åŠå‰ä¸€ä¸ªå’Œåä¸€ä¸ªè¯æ¥ç”ŸæˆåµŒå…¥å¼è¡¨ç¤ºï¼š$\boldsymbol{y}_t = f(\boldsymbol{x}_{t-1},\boldsymbol{x}_t,\boldsymbol{x}_{t+1})$ã€‚CNN èƒ½å¤Ÿå¹¶è¡Œåœ°è®¡ç®—ï¼Œå› æ­¤é€Ÿåº¦å¾ˆå¿«ï¼Œä½†æ˜¯ç”±äºæ˜¯é€šè¿‡çª—å£æ¥è¿›è¡Œç¼–ç ï¼Œæ‰€ä»¥æ›´ä¾§é‡äºæ•è·å±€éƒ¨ä¿¡æ¯ï¼Œéš¾ä»¥å»ºæ¨¡é•¿è·ç¦»çš„è¯­ä¹‰ä¾èµ–ã€‚

Googleã€ŠAttention is All You Needã€‹æä¾›äº†ç¬¬ä¸‰ä¸ªæ–¹æ¡ˆï¼š**ç›´æ¥ä½¿ç”¨ Attention æœºåˆ¶ç¼–ç æ•´ä¸ªæ–‡æœ¬**ã€‚ç›¸æ¯” RNN è¦é€æ­¥é€’å½’æ‰èƒ½è·å¾—å…¨å±€ä¿¡æ¯ï¼ˆå› æ­¤ä¸€èˆ¬ä½¿ç”¨åŒå‘ RNNï¼‰ï¼Œè€Œ CNN å®é™…åªèƒ½è·å–å±€éƒ¨ä¿¡æ¯ï¼Œéœ€è¦é€šè¿‡å±‚å æ¥å¢å¤§æ„Ÿå—é‡ï¼ŒAttention æœºåˆ¶ä¸€æ­¥åˆ°ä½è·å–äº†å…¨å±€ä¿¡æ¯ï¼š$\boldsymbol{y}_t = f(\boldsymbol{x}_t,\boldsymbol{A},\boldsymbol{B})$ã€‚å…¶ä¸­ $\boldsymbol{A},\boldsymbol{B}$ æ˜¯å¦å¤–çš„è¯è¯­åºåˆ—ï¼ˆçŸ©é˜µï¼‰ï¼Œå¦‚æœå– $\boldsymbol{A}=\boldsymbol{B}=\boldsymbol{X}$ å°±ç§°ä¸º Self-Attentionï¼Œå³ç›´æ¥å°† $\boldsymbol{x}_t$ ä¸è‡ªèº«åºåˆ—ä¸­çš„æ¯ä¸ªè¯è¯­è¿›è¡Œæ¯”è¾ƒï¼Œæœ€åç®—å‡º $\boldsymbol{y}_t$ã€‚

| æ¨¡å‹ | å¹¶è¡Œè®¡ç®— | é•¿è·ç¦»ä¾èµ– | ä¼˜ç‚¹ | ç¼ºç‚¹ |
| --- | --- | --- | --- | --- |
| RNN | ğŸ”´ | ğŸŸ¢ | é€’å½’ç»“æ„ï¼Œç±»ä¼¼äººç±»é˜…è¯» | æ— æ³•å¹¶è¡Œè®¡ç®—ï¼Œéš¾ä»¥å­¦ä¹ å…¨å±€ç»“æ„ |
| CNN | ğŸŸ¢ | ğŸ”´ | å¹¶è¡Œè®¡ç®— | å±€éƒ¨ä¿¡æ¯ï¼Œéš¾ä»¥å»ºæ¨¡é•¿è·ç¦»ä¾èµ– |
| Self-Attention | ğŸŸ¢ | ğŸŸ¢ | ä¸€æ­¥åˆ°ä½è·å–å…¨å±€ä¿¡æ¯ | è®¡ç®—å¤æ‚åº¦é«˜ï¼Œéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æº |

## Scaled Dot-product Attention

```mermaid
graph BT;
    Q --> MatMul1;
    K --> MatMul1;
    MatMul1[MatMul] --> Scale;
    Scale --> Mask["Mask (opt.)"];
    Mask --> SoftMax;
    SoftMax --> MatMul2;
    V --> MatMul2;
    MatMul2[MatMul] --> Result;
```

Scaled Dot-product Attention å…±åŒ…å« 2 ä¸ªä¸»è¦æ­¥éª¤ï¼š

1. **è®¡ç®—æ³¨æ„åŠ›æƒé‡**ï¼šä½¿ç”¨æŸç§ç›¸ä¼¼åº¦å‡½æ•°åº¦é‡æ¯ä¸€ä¸ª query å‘é‡å’Œæ‰€æœ‰ key å‘é‡ä¹‹é—´çš„å…³è”ç¨‹åº¦ã€‚å¯¹äºé•¿åº¦ä¸º $m$ çš„ Query åºåˆ—å’Œé•¿åº¦ä¸º $n$ çš„ Key åºåˆ—ï¼Œè¯¥æ­¥éª¤ä¼šç”Ÿæˆä¸€ä¸ªå°ºå¯¸ä¸º $m \times n$ çš„æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µã€‚

    ç‰¹åˆ«åœ°ï¼ŒScaled Dot-product Attention ä½¿ç”¨ç‚¹ç§¯ä½œä¸ºç›¸ä¼¼åº¦å‡½æ•°ï¼Œè¿™æ ·ç›¸ä¼¼çš„ queries å’Œ keys ä¼šå…·æœ‰è¾ƒå¤§çš„ç‚¹ç§¯ã€‚

    ç”±äºç‚¹ç§¯å¯ä»¥äº§ç”Ÿä»»æ„å¤§çš„æ•°å­—ï¼Œè¿™ä¼šç ´åè®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚å› æ­¤æ³¨æ„åŠ›åˆ†æ•°è¿˜éœ€è¦ä¹˜ä»¥ä¸€ä¸ªç¼©æ”¾å› å­æ¥æ ‡å‡†åŒ–å®ƒä»¬çš„æ–¹å·®ï¼Œç„¶åç”¨ä¸€ä¸ª softmax æ ‡å‡†åŒ–ã€‚è¿™æ ·å°±å¾—åˆ°äº†æœ€ç»ˆçš„æ³¨æ„åŠ›æƒé‡ $w_{ij}$ï¼Œè¡¨ç¤ºç¬¬ $i$ ä¸ª query å‘é‡ä¸ç¬¬ $j$ ä¸ª key å‘é‡ä¹‹é—´çš„å…³è”ç¨‹åº¦ã€‚

2. **æ›´æ–° token embeddings**ï¼šå°†æƒé‡ $w_{ij}$ ä¸å¯¹åº”çš„ value å‘é‡ $\boldsymbol{v}_1,â€¦,\boldsymbol{v}_n$ ç›¸ä¹˜ä»¥è·å¾—ç¬¬ $i$ ä¸ª query å‘é‡æ›´æ–°åçš„è¯­ä¹‰è¡¨ç¤º $\boldsymbol{x}_iâ€™ = \sum_{j} w_{ij}\boldsymbol{v}_j$ã€‚

å½¢å¼åŒ–è¡¨ç¤ºä¸ºï¼š

$$
\text{Attention}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = \text{softmax}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^{\top}}{\sqrt{d_k}}\right)\boldsymbol{V} \tag{4}
$$

å…¶ä¸­ $\boldsymbol{Q}\in\mathbb{R}^{m\times d_k}, \boldsymbol{K}\in\mathbb{R}^{n\times d_k}, \boldsymbol{V}\in\mathbb{R}^{n\times d_v}$ åˆ†åˆ«æ˜¯ queryã€keyã€value å‘é‡åºåˆ—ã€‚å¦‚æœå¿½ç•¥ softmax æ¿€æ´»å‡½æ•°ï¼Œå®é™…ä¸Šå®ƒå°±æ˜¯ä¸‰ä¸ª $m\times d_k,d_k\times n, n\times d_v$ çŸ©é˜µç›¸ä¹˜ï¼Œå¾—åˆ°ä¸€ä¸ª $m\times d_v$ çš„çŸ©é˜µï¼Œä¹Ÿå°±æ˜¯å°† $m\times d_k$ çš„åºåˆ— $\boldsymbol{Q}$ ç¼–ç æˆäº†ä¸€ä¸ªæ–°çš„ $m\times d_v$ çš„åºåˆ—ã€‚

å°†ä¸Šé¢çš„å…¬å¼æ‹†å¼€æ¥çœ‹æ›´åŠ æ¸…æ¥šï¼š

$$
\text{Attention}(\boldsymbol{q}_t,\boldsymbol{K},\boldsymbol{V}) = \sum_{s=1}^n \frac{1}{Z}\exp\left(\frac{\langle\boldsymbol{q}_t, \boldsymbol{k}_s\rangle}{\sqrt{d_k}}\right)\boldsymbol{v}_s \tag{5}
$$

å…¶ä¸­ $Z$ æ˜¯å½’ä¸€åŒ–å› å­ï¼Œ$\boldsymbol{K},\boldsymbol{V}$ æ˜¯ä¸€ä¸€å¯¹åº”çš„ key å’Œ value å‘é‡åºåˆ—ï¼ŒScaled Dot-product Attention å°±æ˜¯é€šè¿‡ $\boldsymbol{q}_t$ è¿™ä¸ª query ä¸å„ä¸ª $\boldsymbol{k}_s$ å†…ç§¯å¹¶ softmax çš„æ–¹å¼æ¥å¾—åˆ° $\boldsymbol{q}_t$ ä¸å„ä¸ª $\boldsymbol{v}_s$ çš„ç›¸ä¼¼åº¦ï¼Œç„¶ååŠ æƒæ±‚å’Œï¼Œå¾—åˆ°ä¸€ä¸ª $d_v$ ç»´çš„å‘é‡ã€‚å…¶ä¸­å› å­ $\sqrt{d_k}$ èµ·åˆ°è°ƒèŠ‚ä½œç”¨ï¼Œä½¿å¾—å†…ç§¯ä¸è‡³äºå¤ªå¤§ã€‚
